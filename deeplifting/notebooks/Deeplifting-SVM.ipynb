{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da720180-4028-49ad-9886-f139cc0b71c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ryandevera/data-science/umn_environments/Deeplifting/deeplifting/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ba49b7f-d35d-4d6d-9808-f6cf773c0313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ryandevera/data-science/umn_environments/Deeplifting\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41873e7-5036-4bf0-a8bd-d038da320819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from deeplifting.models import DeepliftingSkipMLP\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.optimize import differential_evolution, dual_annealing\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "\n",
    "# import pygranso functions\n",
    "from pygranso.private.getNvar import getNvarTorch\n",
    "from pygranso.pygranso import pygranso\n",
    "from pygranso.pygransoStruct import pygransoStruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b664e1-d260-4be3-b20c-551efdfb845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# target_names = ['setosa', 'other']\n",
    "# # Apply t-SNE to the data\n",
    "# tsne = TSNE(n_components=2, random_state=0)\n",
    "# X_tsne = tsne.fit_transform(inputs_X.T[:, :-1])\n",
    "\n",
    "# # Create a scatter plot\n",
    "# # Create a scatter plot using matplotlib\n",
    "# colors = ['red', 'blue', 'green']  # Define a color for each class\n",
    "# for i in [-1, 1]:\n",
    "#     plt.scatter(\n",
    "#         X_tsne[predictions == i, 0],\n",
    "#         X_tsne[predictions == i, 1],\n",
    "#         c=colors[i],\n",
    "#         label=target_names[i],\n",
    "#         edgecolors='w',\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16830523-2c8b-4d0e-a431-22c6795776a7",
   "metadata": {},
   "source": [
    "# Let's clean up the SVM notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f5807a-d467-4150-925f-038638329d6e",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8439b85e-8c66-402b-9583-e477c5c5de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a utility for loading in the iris dataset with option for a test set\n",
    "def build_iris_dataset(\n",
    "    num_features=2, species_class=0, test_split=True, torch_version=False\n",
    "):\n",
    "    # Load the dataset\n",
    "    iris = load_iris()\n",
    "\n",
    "    # The data and target labels\n",
    "    data = iris.data\n",
    "    labels = iris.target\n",
    "\n",
    "    # If you want the feature names and target names:\n",
    "    feature_names = iris.feature_names\n",
    "    target_names = iris.target_names\n",
    "\n",
    "    df = pd.DataFrame(data=iris.data, columns=['f1', 'f2', 'f3', 'f4'])\n",
    "    df['f5'] = 1.0\n",
    "    df['labels'] = iris.target\n",
    "\n",
    "    # Resample the data\n",
    "    df = df.sample(frac=1.0).reset_index(drop=True)\n",
    "\n",
    "    # Dimensions\n",
    "    output_size = len(feature_names) + 1\n",
    "\n",
    "    # Change features here\n",
    "    if num_features == 2:\n",
    "        columns = ['f1', 'f2', 'f5']\n",
    "    elif num_features == 5:\n",
    "        columns = ['f1', 'f2', 'f3', 'f4', 'f5']\n",
    "\n",
    "    # Set up the variables\n",
    "    y = df['labels'].values\n",
    "\n",
    "    # Binarize the labels\n",
    "    labels = np.zeros(len(y))\n",
    "    labels[y != species_class] = -1\n",
    "    labels[y == species_class] = 1\n",
    "    y = labels.copy()\n",
    "\n",
    "    X = df[columns].values\n",
    "\n",
    "    # Sample the data into train and test\n",
    "    if test_split:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        if torch_version:\n",
    "            device = torch.device('cpu')\n",
    "            # Torch X and y\n",
    "            X_train = torch.from_numpy(X_train).to(device=device, dtype=torch.double)\n",
    "            X_test = torch.from_numpy(X_test).to(device=device, dtype=torch.double)\n",
    "\n",
    "            # y variables\n",
    "            y_train = torch.from_numpy(y_train).to(device=device, dtype=torch.double)\n",
    "            y_test = torch.from_numpy(y_test).to(device=device, dtype=torch.double)\n",
    "\n",
    "        return {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        X = X.copy()\n",
    "        y = y.copy()\n",
    "\n",
    "        if torch_version:\n",
    "            X = torch.from_numpy(X).to(device=device, dtype=torch.double)\n",
    "            y = torch.from_numpy(y).to(device=device, dtype=torch.double)\n",
    "\n",
    "        return {'X_train': X, 'y_train': y, 'X_test': None, 'y_test': None}\n",
    "\n",
    "\n",
    "def build_mnist_dataset(number_class=0, test_split=True, torch_version=False):\n",
    "    # Load the dataset\n",
    "    # Load the MNIST dataset\n",
    "    digits = load_digits()\n",
    "\n",
    "    # Split the dataset into features and target variable\n",
    "    X = digits.data / 255.0\n",
    "\n",
    "    # If you want the feature names and target names:\n",
    "    feature_names = digits.feature_names\n",
    "\n",
    "    columns = [f'{i + 1}' for i in range(X.shape[1])]\n",
    "    df = pd.DataFrame(data=X, columns=columns)\n",
    "    df[f'f{X.shape[1] + 1}'] = 1.0\n",
    "\n",
    "    # Dimensions\n",
    "    output_size = len(feature_names) + 1\n",
    "\n",
    "    # Set up the variables\n",
    "    X = df.values\n",
    "    y = digits.target\n",
    "\n",
    "    # Binarize the labels\n",
    "    labels = np.zeros(len(y))\n",
    "    labels[y != number_class] = -1\n",
    "    labels[y == number_class] = 1\n",
    "    y = labels.copy()\n",
    "\n",
    "    X = df[columns].values\n",
    "\n",
    "    # Sample the data into train and test\n",
    "    if test_split:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        if torch_version:\n",
    "            device = torch.device('cpu')\n",
    "            # Torch X and y\n",
    "            X_train = torch.from_numpy(X_train).to(device=device, dtype=torch.double)\n",
    "            X_test = torch.from_numpy(X_test).to(device=device, dtype=torch.double)\n",
    "\n",
    "            # y variables\n",
    "            y_train = torch.from_numpy(y_train).to(device=device, dtype=torch.double)\n",
    "            y_test = torch.from_numpy(y_test).to(device=device, dtype=torch.double)\n",
    "\n",
    "        return {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        X = X.copy()\n",
    "        y = y.copy()\n",
    "\n",
    "        if torch_version:\n",
    "            X = torch.from_numpy(X).to(device=device, dtype=torch.double)\n",
    "            y = torch.from_numpy(y).to(device=device, dtype=torch.double)\n",
    "\n",
    "        return {'X_train': X, 'y_train': y, 'X_test': None, 'y_test': None}\n",
    "\n",
    "\n",
    "def build_cifar100_dataset(image_class=46, test_split=True, torch_version=False):\n",
    "    # Transformations applied to the dataset\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "    # Load the CIFAR-100 dataset\n",
    "    cifar100_dataset = datasets.CIFAR100(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    # Split the dataset into features and target variable\n",
    "    X = np.array([image.flatten().numpy() for image, label in cifar100_dataset])\n",
    "    y = np.array([label for _, label in cifar100_dataset])\n",
    "\n",
    "    columns = [f'{i + 1}' for i in range(X.shape[1])]\n",
    "    df = pd.DataFrame(data=X, columns=columns)\n",
    "    df[f'f{X.shape[1] + 1}'] = 1.0\n",
    "    df['labels'] = y\n",
    "\n",
    "    # Need a smaller sample\n",
    "    df = df.sample(frac=0.30)\n",
    "\n",
    "    X = df[columns + [f'f{X.shape[1] + 1}']].values\n",
    "    y = df['labels'].values\n",
    "    labels = np.zeros(len(y))\n",
    "    labels[y == image_class] = 1\n",
    "    labels[y != image_class] = -1\n",
    "    y = labels.copy()\n",
    "\n",
    "    # Sample the data into train and test\n",
    "    if test_split:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        if torch_version:\n",
    "            device = torch.device('cpu')\n",
    "            # Convert X and y to Torch tensors\n",
    "            X_train = torch.tensor(X_train).to(device=device, dtype=torch.double)\n",
    "            X_test = torch.tensor(X_test).to(device=device, dtype=torch.double)\n",
    "            y_train = torch.tensor(y_train).to(device=device, dtype=torch.double)\n",
    "            y_test = torch.tensor(y_test).to(device=device, dtype=torch.double)\n",
    "\n",
    "        return {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        X = X.copy()\n",
    "        y = y.copy()\n",
    "\n",
    "        if torch_version:\n",
    "            device = torch.device('cpu')\n",
    "            X = torch.tensor(X).to(device=device, dtype=torch.float32)\n",
    "            y = torch.tensor(y).to(device=device, dtype=torch.long)\n",
    "\n",
    "        return {'X_train': X, 'y_train': y, 'X_test': None, 'y_test': None}\n",
    "\n",
    "\n",
    "# Set up the learning function this will be for algorithms such\n",
    "# as dual annealing\n",
    "def numpy_svm(weight_vec, inputs_X, labels):\n",
    "    # Compute SVM objective\n",
    "    denominator = np.linalg.norm(weight_vec, ord=2)\n",
    "    prod = np.matmul(weight_vec.T, inputs_X)\n",
    "\n",
    "    numerator = (labels * prod).flatten()\n",
    "    obj = numerator / denominator\n",
    "\n",
    "    # Orig obj\n",
    "    f = np.amax(-1 * obj)\n",
    "    return f\n",
    "\n",
    "\n",
    "# Set up the learning function - this will be for PyGRANSO\n",
    "def pygranso_svm(X_struct, inputs_X, labels):\n",
    "    weight_vec = X_struct.w\n",
    "\n",
    "    # Compute SVM objective\n",
    "    denominator = torch.linalg.norm(weight_vec, ord=2)\n",
    "    prod = torch.matmul(weight_vec.T, inputs_X)\n",
    "    numerator = labels * prod\n",
    "    obj = numerator / denominator\n",
    "\n",
    "    # Orig obj\n",
    "    f = torch.amax(-1 * obj)\n",
    "\n",
    "    ce = None\n",
    "    ci = None\n",
    "    return f, ci, ce\n",
    "\n",
    "\n",
    "# Set up the learning function\n",
    "def deeplifting_svm(model, X, labels):\n",
    "    outputs = model(None)\n",
    "    weight_vec = outputs.mean(axis=0)\n",
    "\n",
    "    # Compute SVM objective\n",
    "    denominator = torch.linalg.norm(weight_vec, ord=2)\n",
    "    prod = torch.matmul(weight_vec.T, X)\n",
    "    numerator = labels * prod\n",
    "    obj = numerator / denominator\n",
    "\n",
    "    # Orig obj\n",
    "    f = torch.amax(-1 * obj)\n",
    "\n",
    "    ce = None\n",
    "    ci = None\n",
    "    return f, ci, ce\n",
    "\n",
    "\n",
    "def run_dual_annealing_svm(X, labels):\n",
    "    # Initialize a weight vector\n",
    "    x0 = np.random.randn(X.shape[0])\n",
    "\n",
    "    # Setup the objective function\n",
    "    fn = lambda w: numpy_svm(w, X, labels)\n",
    "\n",
    "    # For this problem we will set up arbitrary bounds\n",
    "    bounds = [(-10, 10)] * X.shape[0]\n",
    "\n",
    "    # Get the result\n",
    "    result = dual_annealing(\n",
    "        fn,\n",
    "        bounds,\n",
    "        x0=x0,\n",
    "        maxiter=1000,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_pygranso(X, labels):\n",
    "    device = torch.device('cpu')\n",
    "    w0 = torch.randn(\n",
    "        (X.shape[0], 1),\n",
    "    ).to(device, dtype=torch.double)\n",
    "    var_in = {\"w\": list(w0.shape)}\n",
    "\n",
    "    comb_fn = lambda X_struct: pygranso_svm(\n",
    "        X_struct,\n",
    "        X,\n",
    "        labels,\n",
    "    )\n",
    "\n",
    "    opts = pygransoStruct()\n",
    "\n",
    "    # PyGranso options\n",
    "    # Increase max number of iterations and let convege to stationarity\n",
    "    # Do we see local minima in the PyGranso version\n",
    "    # Dual Annealing, SCIP and Deeplifting, PyGranso (showing there are local minima)\n",
    "    opts.x0 = torch.reshape(w0, (-1, 1))\n",
    "    opts.torch_device = device\n",
    "    opts.print_frequency = 10\n",
    "    opts.limited_mem_size = 5\n",
    "    opts.stat_l2_model = False\n",
    "    opts.double_precision = True\n",
    "    opts.opt_tol = 1e-5\n",
    "    opts.maxit = 10000\n",
    "\n",
    "    # Run the main algorithm\n",
    "    soln = pygranso(var_spec=var_in, combined_fn=comb_fn, user_opts=opts)\n",
    "    return soln\n",
    "\n",
    "\n",
    "def run_deeplifting(model, X, labels):\n",
    "    # Deeplifting time!\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device=device, dtype=torch.double)\n",
    "    nvar = getNvarTorch(model.parameters())\n",
    "\n",
    "    opts = pygransoStruct()\n",
    "\n",
    "    # Inital x0\n",
    "    x0 = (\n",
    "        torch.nn.utils.parameters_to_vector(model.parameters())\n",
    "        .detach()\n",
    "        .reshape(nvar, 1)\n",
    "        .to(device=device, dtype=torch.double)\n",
    "    )\n",
    "\n",
    "    # PyGranso options\n",
    "    # Increase max number of iterations and let convege to stationarity\n",
    "    # Do we see local minima in the PyGranso version\n",
    "    # Dual Annealing, SCIP and Deeplifting, PyGranso (showing there are local minima)\n",
    "    opts.x0 = x0\n",
    "    opts.torch_device = device\n",
    "    opts.print_frequency = 100\n",
    "    opts.limited_mem_size = 100\n",
    "    opts.stat_l2_model = False\n",
    "    opts.double_precision = True\n",
    "    opts.opt_tol = 1e-5\n",
    "    opts.maxit = 6000\n",
    "\n",
    "    # Combined function\n",
    "    comb_fn = lambda model: deeplifting_svm(model, X, labels)  # noqa\n",
    "\n",
    "    # Run the main algorithm\n",
    "    soln = pygranso(var_spec=model, combined_fn=comb_fn, user_opts=opts)\n",
    "\n",
    "    return soln\n",
    "\n",
    "\n",
    "def build_predictions(w, X):\n",
    "    predictions = np.sign(w @ X)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a81f6f-44a9-4071-b7dc-dc0aa3e75197",
   "metadata": {},
   "source": [
    "# Load in the Iris dataset (N Features = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0740be33-089a-43de-bc4e-a901a740b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the numpy version\n",
    "data = build_iris_dataset(\n",
    "    num_features=2, species_class=1, test_split=True, torch_version=False\n",
    ")\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# Get data for the torch version\n",
    "# Data for the numpy version\n",
    "data = build_iris_dataset(\n",
    "    num_features=2, species_class=1, test_split=True, torch_version=True\n",
    ")\n",
    "Xt_train = data['X_train']\n",
    "yt_train = data['y_train']\n",
    "Xt_test = data['X_test']\n",
    "yt_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d324ab0-3253-4653-b62d-76a8362a495c",
   "metadata": {},
   "source": [
    "# Run Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22a6e93-32d9-45c1-b029-21de2c6c8b64",
   "metadata": {},
   "source": [
    "# SCIPY Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58357b9d-8878-41f8-b076-81b89e493c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdever120\u001b[0m (\u001b[33mrydevera-umn\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/ryandevera/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key='2080070c4753d0384b073105ed75e1f46669e4bf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c497947-e2e1-47ec-a262-a62b2e275e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ryandevera/data-science/umn_environments/Deeplifting/wandb/run-20231022_102639-exc7tx8o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rydevera-umn/Deeplifting-SVM/runs/exc7tx8o' target=\"_blank\">sparkling-planet-1</a></strong> to <a href='https://wandb.ai/rydevera-umn/Deeplifting-SVM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rydevera-umn/Deeplifting-SVM' target=\"_blank\">https://wandb.ai/rydevera-umn/Deeplifting-SVM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rydevera-umn/Deeplifting-SVM/runs/exc7tx8o' target=\"_blank\">https://wandb.ai/rydevera-umn/Deeplifting-SVM/runs/exc7tx8o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2316712363167392\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sparkling-planet-1</strong> at: <a href='https://wandb.ai/rydevera-umn/Deeplifting-SVM/runs/exc7tx8o' target=\"_blank\">https://wandb.ai/rydevera-umn/Deeplifting-SVM/runs/exc7tx8o</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231022_102639-exc7tx8o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Deeplifting-SVM\",\n",
    ")\n",
    "\n",
    "# Run the dual annealing version\n",
    "dual_annealing_result = run_dual_annealing_svm(X_train.T, y_train)\n",
    "print(dual_annealing_result.fun)\n",
    "da_weights = dual_annealing_result.x\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8e556-0530-4a0a-a9ae-ece551c65351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train accuracy\n",
    "preds_train = build_predictions(da_weights, X_train.T)\n",
    "\n",
    "# Test accuracy\n",
    "preds_test = build_predictions(da_weights, X_test.T)\n",
    "\n",
    "print(\n",
    "    accuracy_score(y_train, preds_train),\n",
    "    accuracy_score(y_test, preds_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ea789-89e0-4a36-9d46-e663ca614ed8",
   "metadata": {},
   "source": [
    "# PyGRANSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae842442-3c19-468c-a409-6de704b1b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pygranso_result = run_pygranso(Xt_train.T, yt_train)\n",
    "pg_weights = pygranso_result.best.x\n",
    "pg_weights = pg_weights.detach().cpu().numpy().reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa161b9-c84e-45e3-900f-7e4f9461759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train accuracy\n",
    "preds_train = build_predictions(pg_weights, X_train.T)\n",
    "\n",
    "# Test accuracy\n",
    "preds_test = build_predictions(pg_weights, X_test.T)\n",
    "\n",
    "print(\n",
    "    accuracy_score(y_train, preds_train.flatten()),\n",
    "    accuracy_score(y_test, preds_test.flatten()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaac414-11ce-4aff-a638-1beec16b262f",
   "metadata": {},
   "source": [
    "# Deeplifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7b2c8-1089-447d-a1c4-29f099fb7b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model outsize\n",
    "model = DeepliftingSkipMLP(\n",
    "    input_size=32,\n",
    "    hidden_sizes=(128,) * 4,\n",
    "    output_size=Xt_train.T.shape[0],\n",
    "    bounds=None,\n",
    "    skip_every_n=1,\n",
    "    activation='sine',\n",
    "    output_activation='sine',\n",
    "    agg_function='identity',\n",
    "    include_bn=True,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "dl_result = run_deeplifting(model, Xt_train.T, yt_train)\n",
    "dl_weights = model(inputs=None)\n",
    "dl_weights = dl_weights.mean(axis=0)\n",
    "dl_weights = dl_weights.detach().cpu().numpy().flatten().reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc482f0a-345e-42b4-877c-b6801e50e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train accuracy\n",
    "preds_train = build_predictions(dl_weights, X_train.T)\n",
    "\n",
    "# Test accuracy\n",
    "preds_test = build_predictions(dl_weights, X_test.T)\n",
    "\n",
    "print(\n",
    "    accuracy_score(y_train, preds_train.flatten()),\n",
    "    accuracy_score(y_test, preds_test.flatten()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd8106b-eb0a-4423-a877-c7943c362ff1",
   "metadata": {},
   "source": [
    "# Landscape of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d1d51-9ef5-44da-90c4-5449a3e8511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the last weight and let's look at the landscape!\n",
    "W2 = dl_weights.flatten()[-1]\n",
    "\n",
    "resolution = 500\n",
    "W0_range = np.linspace(-2, 2, resolution)\n",
    "W1_range = np.linspace(-2, 2, resolution)\n",
    "objective_value = np.zeros((resolution, resolution))\n",
    "\n",
    "for i, W0 in enumerate(W0_range):\n",
    "    for j, W1 in enumerate(W1_range):\n",
    "        W = np.array([W0, W1, W2]).reshape(-1, 1)\n",
    "        objective_value[i, j] = numpy_svm(W, X_train.T, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66b934-c1b4-482e-9360-e992d8ba0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4), sharex=True, sharey=True)\n",
    "\n",
    "# Plot the weights vs. the loss landscale\n",
    "c1 = ax.contourf(\n",
    "    W0_range, W1_range, objective_value, levels=100, cmap='jet', extend='both'\n",
    ")\n",
    "fig.colorbar(c1, orientation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a15e4-ff54-4385-ae4d-a12b788388ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure\n",
    "fig = plt.figure()\n",
    "\n",
    "# Create 3D axis\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the surface\n",
    "ax.plot_surface(W0_range, W1_range, objective_value, cmap='jet')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('3D Surface Plot')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3515b838-1744-4084-8fd7-fb2a74b12302",
   "metadata": {},
   "source": [
    "# Load in Iris (N Features = 5 & Label = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0df6d5-c8b5-4ba1-b6d3-d623dc4fddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the numpy version\n",
    "data = build_iris_dataset(\n",
    "    num_features=5, species_class=1, test_split=True, torch_version=False\n",
    ")\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# Get data for the torch version\n",
    "# Data for the numpy version\n",
    "data = build_iris_dataset(num_features=5, test_split=True, torch_version=True)\n",
    "Xt_train = data['X_train']\n",
    "yt_train = data['y_train']\n",
    "Xt_test = data['X_test']\n",
    "yt_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c4bab-40b0-4aca-92cd-413e6896a13b",
   "metadata": {},
   "source": [
    "# SCIPY Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30732e-7f23-4247-a489-08d3f31cf02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the dual annealing version\n",
    "dual_annealing_result = run_dual_annealing_svm(X_train.T, y_train)\n",
    "da_weights = dual_annealing_result.x\n",
    "\n",
    "# Train accuracy\n",
    "preds_train = build_predictions(da_weights, X_train.T)\n",
    "\n",
    "# Test accuracy\n",
    "preds_test = build_predictions(da_weights, X_test.T)\n",
    "\n",
    "print(\n",
    "    accuracy_score(y_train, preds_train),\n",
    "    accuracy_score(y_test, preds_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a8d9e-c5bc-452b-9b6c-4e7e698b2d42",
   "metadata": {},
   "source": [
    "# PyGRANSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a5b60c-ecb5-4a19-82a5-efd6c3eb5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pygranso_result = run_pygranso(Xt_train.T, yt_train)\n",
    "pg_weights = pygranso_result.best.x\n",
    "pg_weights = pg_weights.detach().cpu().numpy().reshape(1, -1)\n",
    "\n",
    "# Train accuracy\n",
    "preds_train = build_predictions(pg_weights, X_train.T)\n",
    "\n",
    "# Test accuracy\n",
    "preds_test = build_predictions(pg_weights, X_test.T)\n",
    "\n",
    "print(\n",
    "    accuracy_score(y_train, preds_train.flatten()),\n",
    "    accuracy_score(y_test, preds_test.flatten()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e9f09-89b4-45aa-8bfd-06720c006db8",
   "metadata": {},
   "source": [
    "# Deeplifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd5b024-4728-4fb4-aaf6-23dfba55ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model outsize\n",
    "model = DeepliftingSkipMLP(\n",
    "    input_size=32,\n",
    "    hidden_sizes=(128,) * 3,\n",
    "    output_size=Xt_train.T.shape[0],\n",
    "    bounds=None,\n",
    "    skip_every_n=1,\n",
    "    activation='leaky_relu',\n",
    "    output_activation='sine',\n",
    "    agg_function='identity',\n",
    "    include_bn=True,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "dl_result = run_deeplifting(model, Xt_train.T, yt_train)\n",
    "dl_weights = model(inputs=None)\n",
    "dl_weights = dl_weights.mean(axis=0)\n",
    "dl_weights = dl_weights.detach().cpu().numpy().flatten().reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c8cfa-bb23-4dec-bcb3-c9e0730c5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train accuracy\n",
    "preds_train = build_predictions(dl_weights, X_train.T)\n",
    "\n",
    "# Test accuracy\n",
    "preds_test = build_predictions(dl_weights, X_test.T)\n",
    "\n",
    "print(\n",
    "    accuracy_score(y_train, preds_train.flatten()),\n",
    "    accuracy_score(y_test, preds_test.flatten()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286edd1-dd49-4961-9e43-5a908018a5f6",
   "metadata": {},
   "source": [
    "# Let's work on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71230188-33e5-4766-b452-fd108e330e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = build_mnist_dataset(number_class=0, test_split=True, torch_version=False)\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# Torch data\n",
    "data = build_mnist_dataset(number_class=0, test_split=True, torch_version=True)\n",
    "Xt_train = data['X_train']\n",
    "yt_train = data['y_train']\n",
    "Xt_test = data['X_test']\n",
    "yt_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d5356c-2568-4791-843d-7f21de564b06",
   "metadata": {},
   "source": [
    "# SCIPY Methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0613c-4e2c-4540-8260-20edc7c90b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the dual annealing version\n",
    "dual_annealing_result = run_dual_annealing_svm(X_train.T, y_train)\n",
    "da_weights = dual_annealing_result.x\n",
    "\n",
    "# Train accuracy\n",
    "preds_train = build_predictions(da_weights, X_train.T)\n",
    "\n",
    "# Test accuracy\n",
    "preds_test = build_predictions(da_weights, X_test.T)\n",
    "\n",
    "print(\n",
    "    accuracy_score(y_train, preds_train),\n",
    "    accuracy_score(y_test, preds_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051bac30-d9ea-4157-90b5-9f90bbc1cc79",
   "metadata": {},
   "source": [
    "# PyGRANSO Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e3ca5-da38-411d-8b61-dd35cc5a3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "pygranso_result = run_pygranso(Xt_train.T, yt_train)\n",
    "pg_weights = pygranso_result.best.x\n",
    "pg_weights = pg_weights.detach().cpu().numpy().reshape(1, -1)\n",
    "\n",
    "# Train accuracy\n",
    "preds_train = build_predictions(pg_weights, X_train.T)\n",
    "\n",
    "# Test accuracy\n",
    "preds_test = build_predictions(pg_weights, X_test.T)\n",
    "\n",
    "print(\n",
    "    accuracy_score(y_train, preds_train.flatten()),\n",
    "    accuracy_score(y_test, preds_test.flatten()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2edc72-91b3-468a-a1bd-fb5ebe85c7b6",
   "metadata": {},
   "source": [
    "# Deeplifting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea75dbb-fab7-40c5-96b5-c02127a295c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model outsize\n",
    "model = DeepliftingSkipMLP(\n",
    "    input_size=32,\n",
    "    hidden_sizes=(128,) * 3,\n",
    "    output_size=Xt_train.T.shape[0],\n",
    "    bounds=None,\n",
    "    skip_every_n=1,\n",
    "    activation='leaky_relu',\n",
    "    output_activation='sine',\n",
    "    agg_function='identity',\n",
    "    include_bn=True,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "dl_result = run_deeplifting(model, Xt_train.T, yt_train)\n",
    "dl_weights = model(inputs=None)\n",
    "dl_weights = dl_weights.mean(axis=0)\n",
    "dl_weights = dl_weights.detach().cpu().numpy().flatten().reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f0519-056a-4dc4-b25d-def9f72d1801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train accuracy\n",
    "preds_train = build_predictions(dl_weights, X_train.T)\n",
    "\n",
    "# Test accuracy\n",
    "preds_test = build_predictions(dl_weights, X_test.T)\n",
    "\n",
    "print(\n",
    "    accuracy_score(y_train, preds_train.flatten()),\n",
    "    accuracy_score(y_test, preds_test.flatten()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf8285-6c44-4470-8e14-3b0f4f353f26",
   "metadata": {},
   "source": [
    "# CIFAR 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c9dac1-195c-480d-a1ff-ef513a15e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy data\n",
    "data = build_cifar100_dataset(image_class=46, test_split=True, torch_version=False)\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# Torch data\n",
    "data = build_cifar100_dataset(image_class=46, test_split=True, torch_version=True)\n",
    "Xt_train = data['X_train']\n",
    "yt_train = data['y_train']\n",
    "Xt_test = data['X_test']\n",
    "yt_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3bfd62-90df-40cc-ad3c-57888346349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifty the shapes of the data\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af94af97-702a-4f79-a827-6a20460409e9",
   "metadata": {},
   "source": [
    "# SCIPY Methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0aae36-74a4-438e-9dd8-86d8b1569011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the dual annealing version\n",
    "dual_annealing_result = run_dual_annealing_svm(X_train.T, y_train)\n",
    "da_weights = dual_annealing_result.x\n",
    "\n",
    "# Train accuracy\n",
    "preds_train = build_predictions(da_weights, X_train.T)\n",
    "\n",
    "# Test accuracy\n",
    "preds_test = build_predictions(da_weights, X_test.T)\n",
    "\n",
    "print(\n",
    "    accuracy_score(y_train, preds_train),\n",
    "    accuracy_score(y_test, preds_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7d6395-a677-458d-9965-5f39a800b8d4",
   "metadata": {},
   "source": [
    "# PyGRANSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ce196-ce83-46dd-9206-08ba430c7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "pygranso_result = run_pygranso(Xt_train.T, yt_train)\n",
    "pg_weights = pygranso_result.best.x\n",
    "pg_weights = pg_weights.detach().cpu().numpy().reshape(1, -1)\n",
    "\n",
    "# Train accuracy\n",
    "preds_train = build_predictions(pg_weights, X_train.T)\n",
    "\n",
    "# Test accuracy\n",
    "preds_test = build_predictions(pg_weights, X_test.T)\n",
    "\n",
    "print(\n",
    "    accuracy_score(y_train, preds_train.flatten()),\n",
    "    accuracy_score(y_test, preds_test.flatten()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea308fc-9fc2-49f8-b629-4b270145bd1c",
   "metadata": {},
   "source": [
    "# Deeplifting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c8b82-8d9e-4ef1-87dd-aff27118dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model outsize\n",
    "model = DeepliftingSkipMLP(\n",
    "    input_size=64,\n",
    "    hidden_sizes=(128,) * 3,\n",
    "    output_size=Xt_train.T.shape[0],\n",
    "    bounds=None,\n",
    "    skip_every_n=1,\n",
    "    activation='leaky_relu',\n",
    "    output_activation='sine',\n",
    "    agg_function='identity',\n",
    "    include_bn=True,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "dl_result = run_deeplifting(model, Xt_train.T, yt_train)\n",
    "dl_weights = model(inputs=None)\n",
    "dl_weights = dl_weights.mean(axis=0)\n",
    "dl_weights = dl_weights.detach().cpu().numpy().flatten().reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f30d00-66ab-4278-8e73-555b59252b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train accuracy\n",
    "preds_train = build_predictions(dl_weights, X_train.T)\n",
    "\n",
    "# Test accuracy\n",
    "preds_test = build_predictions(dl_weights, X_test.T)\n",
    "\n",
    "print(\n",
    "    accuracy_score(y_train, preds_train.flatten()),\n",
    "    accuracy_score(y_test, preds_test.flatten()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28fc668-fcfa-49eb-a922-7e733e4c0ed8",
   "metadata": {},
   "source": [
    "# Diagnostics for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a37a845b-9660-43b0-972c-fcac373d9148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>values</th>\n",
       "      <th>metric</th>\n",
       "      <th>trial</th>\n",
       "      <th>iteration</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.793904</td>\n",
       "      <td>Objective</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>38402.284719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.454346</td>\n",
       "      <td>Objective</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>38402.284719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.191287</td>\n",
       "      <td>Objective</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>38402.284719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.981433</td>\n",
       "      <td>Objective</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>38402.284719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.975419</td>\n",
       "      <td>Objective</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>38402.284719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     values     metric  trial  iteration problem_name    total_time\n",
       "0  1.793904  Objective      0          0    CIFAR-100  38402.284719\n",
       "1  1.454346  Objective      0          1    CIFAR-100  38402.284719\n",
       "2  1.191287  Objective      0          2    CIFAR-100  38402.284719\n",
       "3  0.981433  Objective      0          3    CIFAR-100  38402.284719\n",
       "4  0.975419  Objective      0          4    CIFAR-100  38402.284719"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = os.path.join(\n",
    "    os.getcwd(), 'data-queue-2023-09-24', 'svm', 'svm-deeplifting-results.parquet'\n",
    ")\n",
    "svm_deeplifting_df = pd.read_parquet(save_path)\n",
    "svm_deeplifting_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc881bdb-febb-4c56-a6af-1dd19b3865f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trial\n",
       "0    30003\n",
       "1      240\n",
       "2        9\n",
       "3       21\n",
       "4       36\n",
       "5       21\n",
       "6       30\n",
       "7      144\n",
       "8       78\n",
       "9      237\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_deeplifting_df.groupby('trial').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5f6c5-369e-49af-b74a-467df1f793b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(9, 4))\n",
    "sns.lineplot(\n",
    "    data=svm_deeplifting_df,\n",
    "    x='iteration',\n",
    "    y='values',\n",
    "    hue='metric',\n",
    "    ax=ax,\n",
    ")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f58171-aaa6-47da-b469-928ddaf1303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_final_df = svm_deeplifting_df.groupby(['trial', 'metric']).last().reset_index()\n",
    "svm_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2cbcff-31d0-4bab-8e1e-4696829c6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "sns.barplot(\n",
    "    data=svm_final_df, x='trial', y='values', hue='metric', palette='tab10_r', ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "401dc5d8-6123-4c09-ab93-1071f5821a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_annealing_file_path = (\n",
    "    './experiments/50a4cfe4-16d5-475d-b074-6ae6c046250e/svm/dual-annealing/svm.parquet'\n",
    ")\n",
    "data = pd.read_parquet(dual_annealing_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81e08a0f-2a5b-48d8-8f3a-b9c3821c415a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>values</th>\n",
       "      <th>metric</th>\n",
       "      <th>trial</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.128143</td>\n",
       "      <td>Objective</td>\n",
       "      <td>0</td>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>293.481414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.609875</td>\n",
       "      <td>Train-Accuracy</td>\n",
       "      <td>0</td>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>293.481414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.608500</td>\n",
       "      <td>Test-Accuracy</td>\n",
       "      <td>0</td>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>293.481414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     values          metric  trial problem_name  total_time\n",
       "0  1.128143       Objective      0    CIFAR-100  293.481414\n",
       "1  0.609875  Train-Accuracy      0    CIFAR-100  293.481414\n",
       "2  0.608500   Test-Accuracy      0    CIFAR-100  293.481414"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ffa96c-d678-43ca-b26e-4c167722f834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplifting",
   "language": "python",
   "name": "deeplifting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
